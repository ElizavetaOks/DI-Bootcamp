{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Daily Challenge: Creating A Text Generator"
      ],
      "metadata": {
        "id": "ciTXs7G0L7SQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries And Load Data:"
      ],
      "metadata": {
        "id": "pZ_fKTJ1Tb_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e9y8coIYL6of"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "import numpy as np\n",
        "import nltk\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulzk4y6XMrxL",
        "outputId": "3efd7f6d-360b-4134-b1b4-6a2a73940a4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the text from the given URL\n",
        "url = \"https://www.gutenberg.org/cache/epub/11/pg11.txt\"\n",
        "response = requests.get(url)\n",
        "raw_text = response.text"
      ],
      "metadata": {
        "id": "cEaLhZA4MCeF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Text Data:"
      ],
      "metadata": {
        "id": "vIDBVNm3Tg38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove non-alphabetic characters and extra whitespaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "gCYbpgokUpNK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into sentences\n",
        "def split_text_into_sentences(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "Df0ACzkKVG5A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the raw text\n",
        "cleaned_text = preprocess_text(raw_text)\n",
        "sentences = split_text_into_sentences(cleaned_text)"
      ],
      "metadata": {
        "id": "TiGj0cEORxYk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 200 characters of the corpus\n",
        "corpus = \" \".join(sentences)\n",
        "print(\"First 200 characters of the corpus:\")\n",
        "print(corpus[:200])"
      ],
      "metadata": {
        "id": "fH6XL7meRyL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d72460-fdd9-4f6c-de9d-46bac3e52fa6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 200 characters of the corpus:\n",
            "The Project Gutenberg eBook of Alices Adventures in Wonderland This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary and calculate total_words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"\\nTotal number of words in the vocabulary:\", total_words)"
      ],
      "metadata": {
        "id": "LiXrRieXRz0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590d940d-b4e5-400b-ed6c-b89d28316828"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of words in the vocabulary: 3193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Input And Output Data:"
      ],
      "metadata": {
        "id": "90kxPETvaw5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create n-gram sequences and pad the input data\n",
        "input_sequence = []\n",
        "seq_length = 50\n",
        "\n",
        "for sentence in sentences:\n",
        "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    for i in range(1, len(tokenized_sentence)):\n",
        "        n_gram_sequence = tokenized_sentence[:i+1]\n",
        "        input_sequence.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max([len(seq) for seq in input_sequence])"
      ],
      "metadata": {
        "id": "TXiL5Oi0TMGE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Pad sequences\n",
        "padded_input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "# Display a sample of the padded input sequence\n",
        "print(\"Sample of the padded input sequence:\")\n",
        "print(padded_input_sequence[:5])"
      ],
      "metadata": {
        "id": "ntlBJpHqbEaM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b91999d-4569-4363-be29-96d433af1e47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of the padded input sequence:\n",
            "[[  0   0   0 ...   0   1  46]\n",
            " [  0   0   0 ...   1  46  47]\n",
            " [  0   0   0 ...  46  47 300]\n",
            " [  0   0   0 ...  47 300   5]\n",
            " [  0   0   0 ... 300   5 236]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build The Neural Network Model:"
      ],
      "metadata": {
        "id": "rHOWPtZPk6aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add Embedding layer for text representation\n",
        "embedding_dim = 22\n",
        "model.add(Embedding(input_dim=total_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(30, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(30))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(total_words, activation='softmax'))"
      ],
      "metadata": {
        "id": "o2-y_2PolFsq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile And Train The Model:"
      ],
      "metadata": {
        "id": "blgUy_RLm5d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QSvf0fQbFcJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d299d7-47c5-4e1b-e3ca-e7a91c4c1d6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 29463, 22)         70246     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 29463, 30)         6360      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 29463, 30)         0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 30)                7320      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 30)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3193)              98983     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 182909 (714.49 KB)\n",
            "Trainable params: 182909 (714.49 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model.fit(padded_input_sequence[:, :-1], padded_input_sequence[:, -1], epochs=20, validation_split=0.2, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "VdXIQBpGGHMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_input_sequence[:, :-1], padded_input_sequence[:, -1])\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "9VlySgGJIASZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create generate_text() function\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_length):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list)\n",
        "        predicted_word_index = np.argmax(predicted_probs)\n",
        "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
        "        seed_text += \" \" + predicted_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "4l5nzKRuIEQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of generate_text()\n",
        "seed_text = \"Alice\"\n",
        "generated_text = generate_text(seed_text, next_words=10, model=model, tokenizer=tokenizer, max_sequence_length=max_sequence_length)\n",
        "print(\"Generated Text:\", generated_text)"
      ],
      "metadata": {
        "id": "6CfLHo7gJNx5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}